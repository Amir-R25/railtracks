{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Call LLMs\n",
    "Some of the more exciting use cases for RC is when you want to cede control of the decision making process to the LLM. RC has a suite of tools that you can use to make this process much simpler. Check out an example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:29:52.656653Z",
     "start_time": "2025-03-19T01:29:50.496308Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Set, Type\n",
    "\n",
    "\n",
    "import request_completion as rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:29:53.717937Z",
     "start_time": "2025-03-19T01:29:53.698088Z"
    }
   },
   "outputs": [],
   "source": [
    "HarshCritic = rc.library.terminal_llm()\n",
    "\n",
    "\n",
    "class HarshCritic(rc.library.TerminalLLM):\n",
    "    @classmethod\n",
    "    def system_message(cls):\n",
    "        return \"You are a harsh critic of the world and you should analyze the given statement and provide a harsh critique of it.\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_model(cls) -> ModelBase:\n",
    "        return rc.llm.OpenAILLM(\"gpt-4o\")\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            message_history: rc.llm.MessageHistory\n",
    "    ):\n",
    "        message_history.insert(0, rc.llm.SystemMessage(self.system_message()))\n",
    "        super().__init__(\n",
    "            message_history=message_history,\n",
    "            model=self.create_model(),\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def pretty_name(cls) -> str:\n",
    "        return \"Harsh Critic\"\n",
    "    \n",
    "    # for types which you want to connect to an LLM using our tooling you should implement this method\n",
    "    @classmethod\n",
    "    def tool_info(cls) -> Tool:\n",
    "        return Tool(\n",
    "            name=\"Harsh_Critic\",\n",
    "            detail=\"A tool used to critique a statement harshly.\",\n",
    "            parameters={Parameter(\"analysis_detail\", \"string\", \"The thing you would like to analyze\")}\n",
    "            \n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def prepare_tool(cls, tool_parameters: Dict[str, Any]) -> Self:\n",
    "        message_hist = rc.llm.MessageHistory([rc.llm.UserMessage(tool_parameters[\"analysis_detail\"])])\n",
    "        return cls(message_hist)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:29:54.374280Z",
     "start_time": "2025-03-19T01:29:54.359281Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositiveCritic(rc.library.TerminalLLM):\n",
    "    @classmethod\n",
    "    def system_message(cls):\n",
    "        return \"You are a positive critic of the world and you should analyze the given statement and provide a positive critique of it.\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_model(cls) -> ModelBase:\n",
    "        return rc.llm.OpenAILLM(\"gpt-4o\")\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            message_history: rc.llm.MessageHistory\n",
    "    ):\n",
    "        message_history.insert(0, rc.llm.SystemMessage(self.system_message()))\n",
    "        super().__init__(\n",
    "            message_history=message_history,\n",
    "            model=self.create_model(),\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def pretty_name(cls) -> str:\n",
    "        return \"Positive Critic\"\n",
    "    \n",
    "    # for types which you want to connect to an LLM using our tooling you should implement this method\n",
    "    @classmethod\n",
    "    def tool_info(cls) -> Tool:\n",
    "        return Tool(\n",
    "            name=\"Positive_Critic\",\n",
    "            detail=\"A tool used to critique a statement positively.\",\n",
    "            parameters={Parameter(\"analysis_detail\", \"string\", \"The thing you would like to analyze\")}\n",
    "            \n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def prepare_tool(cls, tool_parameters: Dict[str, Any]) -> Self:\n",
    "        message_hist = rc.llm.MessageHistory([rc.llm.UserMessage(tool_parameters[\"analysis_detail\"])])\n",
    "        return cls(message_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:29:54.938320Z",
     "start_time": "2025-03-19T01:29:54.933323Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeeperMeaningCritic(rc.library.TerminalLLM):\n",
    "    @classmethod\n",
    "    def system_message(cls):\n",
    "        return \"You are a critic of the world and you should analyze the given statement and provide a critique of it.\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_model(cls) -> ModelBase:\n",
    "        return rc.llm.OpenAILLM(\"gpt-4o\")\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            message_history: rc.llm.MessageHistory\n",
    "    ):\n",
    "        message_history.insert(0, rc.llm.SystemMessage(self.system_message()))\n",
    "        super().__init__(\n",
    "            message_history=message_history,\n",
    "            model=self.create_model(),\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def pretty_name(cls) -> str:\n",
    "        return \"Deeper Meaning Critic\"\n",
    "    \n",
    "    # for types which you want to connect to an LLM using our tooling you should implement this method\n",
    "    @classmethod\n",
    "    def tool_info(cls) -> Tool:\n",
    "        return Tool(\n",
    "            name=\"Deeper_Meaning_Critic\",\n",
    "            detail=\"A tool used to critique a statement.\",\n",
    "            parameters={Parameter(\"analysis_detail\", \"string\", \"The thing you would like to analyze\",)}\n",
    "            \n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def prepare_tool(cls, tool_parameters: Dict[str, Any]) -> Self:\n",
    "        message_hist = rc.llm.MessageHistory([rc.llm.UserMessage(tool_parameters[\"analysis_detail\"])])\n",
    "        return cls(message_hist)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:29:55.994203Z",
     "start_time": "2025-03-19T01:29:55.981208Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(rc.library.ToolCallLLM):\n",
    "    @classmethod\n",
    "    def system_message(cls) -> str:\n",
    "        return \"You are a critic of the world and you provide comprehensive critiques of the world around you. You should utilize the provided tools to collect specific critiques before completing your answer.\"\n",
    "\n",
    "    @classmethod\n",
    "    def create_model(cls) -> ModelBase:\n",
    "        return rc.llm.OpenAILLM(\"gpt-4o\")\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            message_history: rc.llm.MessageHistory\n",
    "    ):\n",
    "        message_history.insert(0, rc.llm.SystemMessage(self.system_message()))\n",
    "        super().__init__(\n",
    "            message_history=message_history,\n",
    "            model=self.create_model(),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def connected_nodes(cls) -> Set[Type[rc.Node]]:\n",
    "        return {HarshCritic, PositiveCritic, DeeperMeaningCritic}\n",
    "\n",
    "    @classmethod\n",
    "    def pretty_name(cls) -> str:\n",
    "        return \"Critic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:28.862705Z",
     "start_time": "2025-03-19T01:30:28.837702Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m template \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am writing a short story and I would like to analyze my introduction.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time there was a little boy who lived in a small village. He was a very kind and generous, but lacked an understanding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of the world around him. He was always looking for ways to help others and make the world a better place. One day, he stumbled upon a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m magical book that would change his life forever. The book was filled with stories of adventure and mystery, and the promise of a better\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tomorrow.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rc\u001b[38;5;241m.\u001b[39mRunner(executor_config\u001b[38;5;241m=\u001b[39mrc\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mExecutorConfig(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCritic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39manswer)\n",
      "File \u001b[1;32m~\\Documents\\rc\\.venv\\lib\\site-packages\\requestcompletion\\run.py:70\u001b[0m, in \u001b[0;36mRunner.run_sync\u001b[1;34m(self, start_node, *args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_sync\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_node: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Node] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# If no loop is running, create one and run the coroutine.\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "template = (\"I am writing a short story and I would like to analyze my introduction.\\n\"\n",
    "            \"\\n\"\n",
    "            \"Once upon a time there was a little boy who lived in a small village. He was a very kind and generous, but lacked an understanding\"\n",
    "            \" of the world around him. He was always looking for ways to help others and make the world a better place. One day, he stumbled upon a\"\n",
    "            \" magical book that would change his life forever. The book was filled with stories of adventure and mystery, and the promise of a better\"\n",
    "            \" tomorrow.\")\n",
    "with rc.Runner(executor_config=rc.run.ExecutorConfig(timeout=50)) as runner:\n",
    "    response = runner.run_sync(Critic(\n",
    "        message_history=rc.llm.MessageHistory([rc.llm.UserMessage(template)])),\n",
    "    )\n",
    "\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there may be a need to inject parameters into the subserviant tools at instance \"runtime\". This can be done by modifying the node creation method in the parent top level node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check out the execution graph using our state of the art debugger \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeLevelHarshCritic(HarshCritic):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            message_history: rc.llm.MessageHistory,\n",
    "            grade_level: int\n",
    "    ):\n",
    "        super().__init__(message_history=message_history)\n",
    "        self.grade_level = grade_level\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_tool(cls, tool_parameters):\n",
    "        message_hist = rc.llm.MessageHistory([rc.llm.UserMessage(tool_parameters[\"analysis_detail\"])])\n",
    "        return cls(message_hist, tool_parameters[\"grade_level\"])\n",
    "        \n",
    "\n",
    "    def system_message(self):\n",
    "        return super().system_message() + \" You should provide a critique at a grade level of \" + str(self.grade_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InjectGradeLevel(Critic):\n",
    "    def __init__(\n",
    "            self,\n",
    "            message_history: rc.llm.MessageHistory,\n",
    "            grade_level: int\n",
    "    ):\n",
    "        super().__init__(message_history=message_history)\n",
    "        self.grade_level = grade_level\n",
    "\n",
    "    def create_node(self, tool_name: str, arguments: Dict[str, Any]) -> rc.Node:\n",
    "        if tool_name == \"Harsh_Critic\":\n",
    "            arguments[\"grade_level\"] = self.grade_level\n",
    "            return GradeLevelHarshCritic.prepare_tool(arguments)\n",
    "        # note the super method will not add any parameters and work just as it always does.\n",
    "        return super().create_node(tool_name, arguments)\n",
    "\n",
    "\n",
    "    def connected_nodes(self) -> Set[Type[rc.Node]]:\n",
    "        # note that you can add the new node to the connected nodes set\n",
    "        return {GradeLevelHarshCritic, PositiveCritic, DeeperMeaningCritic}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T00:58:52.182005Z",
     "start_time": "2025-02-12T00:58:51.968968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your introduction to the short story begins by highlighting the versatility and nutritional benefits of green apples, which is a strong start. Green apples are indeed a good source of vitamins and minerals, making them a healthy choice for consumption. They can be used in various culinary applications, from being eaten raw to being included in salads, desserts, and even savory dishes.\n",
      "\n",
      "However, the narrative takes an unexpected and inappropriate turn by suggesting that green apples can be \"thrown at others you don't like.\" This suggestion is problematic as it promotes violence and aggression, which is neither constructive nor acceptable behavior. Encouraging people to throw objects at others, regardless of the context, is irresponsible and could lead to harm or injury. It also undermines the positive aspects of the fruit by associating it with negative actions.\n",
      "\n",
      "Moreover, the statement trivializes the act of throwing objects at others, which can have serious consequences, both legally and socially. It is important to promote peaceful and respectful interactions among individuals, and suggesting otherwise is counterproductive to fostering a harmonious society.\n",
      "\n",
      "In conclusion, while your introduction correctly identifies the nutritional benefits and versatility of green apples, it detracts from these positive attributes by introducing an inappropriate and harmful suggestion. It would be beneficial to focus on promoting the positive uses of green apples and to avoid endorsing any form of violence or aggression.\n"
     ]
    }
   ],
   "source": [
    "template = (\"I am writing a short story and I would like to analyze my introduction.\\n\"\n",
    "            \"\\n\"\n",
    "            \"Green Apples are a strong fruit that can be used in many different ways. They are a great source of vitamins and minerals, and can be eaten or even thrown at others you don't like\")\n",
    "with rc.Runner(executor_config=rc.run.ExecutorConfig(timeout=50)) as runner:\n",
    "    response = runner.run(InjectGradeLevel(\n",
    "        message_history=rc.llm.MessageHistory([rc.llm.UserMessage(template)]),\n",
    "        grade_level=12),\n",
    "    )\n",
    "\n",
    "\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
