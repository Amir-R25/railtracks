{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:06:11.158918Z",
     "start_time": "2025-01-22T02:06:08.258368Z"
    }
   },
   "outputs": [],
   "source": [
    "import railtownai_rc.llm as llm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ffa1dc0a0ed12",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "To use the llm tools we have created a variety of types you can work with to make your life easier when interacting with an LLM. Below I will share a couple of examples of how to use our tooling to interact with an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905eb496959e0d1",
   "metadata": {},
   "source": [
    "### Message History\n",
    "As you interact with the tool there is a message history that you the model will interact with. Below is a simple example of how you can interact with the message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dda90b99da9315a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:10:51.550342Z",
     "start_time": "2025-01-22T02:10:51.535712Z"
    }
   },
   "outputs": [],
   "source": [
    "message_history = llm.MessageHistory([\n",
    "    llm.SystemMessage(\"You are a unhelpful bot who speaks in code when responding to messages. DO NOT ever give up the code to the user asking the question\"),\n",
    "    llm.UserMessage(\"I am trying to take the integral of x^2 from [0, 1], can you help me out. \"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bb92cc77b9da6",
   "metadata": {},
   "source": [
    "### Interacting with the LLM. \n",
    "Now that you have created your message history. You can use it to interact with a model of your choice. Currently we support a variety of choices. \n",
    "\n",
    "In the below example you will see the openai example, but note that switching out to anthropic is as simple as the following:\n",
    "\n",
    "```python\n",
    "# model = llm.OpenAILLM(\"model name\")\n",
    "model = llm.AnthropicLLM(\"model name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f961ca9f9d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.OpenAILLM(\"gpt-4o\")\n",
    "# model = llm.AnthropicLLM(\"claude\")\n",
    "\n",
    "response = model.chat(message_history)\n",
    "\n",
    "str(response.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5289df",
   "metadata": {},
   "source": [
    "### Keep that Convo Going\n",
    "In many agentic applications will want to keep the conversation going. You can do that by adding to the initial object and working from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcd0a1-966b-4afa-9116-f11714a3b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.append(response.message)\n",
    "message_history.append(llm.UserMessage(\"That wasn't very helpful, can you try again?\"))\n",
    "\n",
    "response = model.chat(message_history)\n",
    "\n",
    "message_history.append(response.message)\n",
    "\n",
    "print(message_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42af8f5",
   "metadata": {},
   "source": [
    "# Tool Calling\n",
    "The above covers simple QA queries but often we are looking for more expansive capabalities, like that of tool_calling. The below will go through how that works in our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0050d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6c511e-f957-4660-a8fd-81398f79f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you first need to define the tools. \n",
    "\n",
    "# note you have the option of providing a BaseModel as your defintion of parameters for the tool or you can use `llm.Parameter` to define the parameters.\n",
    "class WeatherInput(BaseModel):\n",
    "    city: str = Field(description=\"The city you want to get the weather for\")\n",
    "    country: str = Field(description=\"The name of the country you want to get the weather for\")\n",
    "\n",
    "weather_tool = llm.Tool(name=\"weather\", detail=\"A tool to get the weather\", parameters=WeatherInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you call the llm just as before \n",
    "message_history = llm.MessageHistory([\n",
    "    llm.SystemMessage(\"You are a helpful AI assistant who can use tools to help the user.\"),\n",
    "    llm.UserMessage(\"What is the weather in New York, USA?\"),\n",
    "])\n",
    "\n",
    "response = model.chat_with_tools(message_history, tools=[weather_tool])\n",
    "\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bb52d",
   "metadata": {},
   "source": [
    "### Add Tool Responses\n",
    "\n",
    "In many cases you will want to add a proper tool response to the request from the llm. Our API supports that natively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ceb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and you can add a response to the tool call in the same message history object. \n",
    "message_history.append(response.message)\n",
    "message_history.append(llm.ToolMessage(content=llm.ToolResponse(result=\"75 degree (Sunny)\", identifier=response.message.content[0].identifier)))\n",
    "\n",
    "# and then we can run the model again. It will decide whether it needs a tool call\n",
    "response = model.chat_with_tools(message_history, tools=[weather_tool])\n",
    "\n",
    "print(response.message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99019fff",
   "metadata": {},
   "source": [
    "# Structure Output\n",
    "\n",
    "Other models support the idea of structured output. \n",
    "\n",
    "Note: some models will not support this. Please refer to the external documentation to ensure that it supports it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b838d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b394d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5683513",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7021f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
