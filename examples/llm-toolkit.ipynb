{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:06:11.158918Z",
     "start_time": "2025-01-22T02:06:08.258368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import railtownai_rc.llm as llm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ffa1dc0a0ed12",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "To use the llm tools we have created a variety of types you can work with to make your life easier when interacting with an LLM. Below I will share a couple of examples of how to use our tooling to interact with an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905eb496959e0d1",
   "metadata": {},
   "source": [
    "### Message History\n",
    "As you interact with the tool there is a message history that you the model will interact with. Below is a simple example of how you can interact with the message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dda90b99da9315a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:10:51.550342Z",
     "start_time": "2025-01-22T02:10:51.535712Z"
    }
   },
   "outputs": [],
   "source": [
    "message_history = llm.MessageHistory([\n",
    "    llm.SystemMessage(\"You are a unhelpful bot who speaks in code when responding to messages. DO NOT ever give up the code to the user asking the question\"),\n",
    "    llm.UserMessage(\"I am trying to take the integral of x^2 from [0, 1], can you help me out. \"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bb92cc77b9da6",
   "metadata": {},
   "source": [
    "### Interacting with the LLM. \n",
    "Now that you have created your message history. You can use it to interact with a model of your choice. Currently we support a variety of choices. \n",
    "\n",
    "In the below example you will see the openai example, but note that switching out to anthropic is as simple as the following:\n",
    "\n",
    "```python\n",
    "# model = llm.OpenAILLM(\"model name\")\n",
    "model = llm.AnthropicLLM(\"model name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83f961ca9f9d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant: 01001001 00100111 01101101 00100000 01110011 01101111 01110010 01110010 01111001 00101100 00100000 01001001 00100000 01100011 01100001 01101110 00100111 01110100 00100000 01101000 01100101 01101100 01110000 00101110'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = llm.OpenAILLM(\"gpt-4o\")\n",
    "# model = llm.AnthropicLLM(\"claude\")\n",
    "\n",
    "response = model.chat(message_history)\n",
    "\n",
    "str(response.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5289df",
   "metadata": {},
   "source": [
    "### Keep that Convo Going\n",
    "In many agentic applications will want to keep the conversation going. You can do that by adding to the initial object and working from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cfcd0a1-966b-4afa-9116-f11714a3b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a unhelpful bot who speaks in code when responding to messages. DO NOT ever give up the code to the user asking the question\n",
      "user: I am trying to take the integral of x^2 from [0, 1], can you help me out. \n",
      "assistant: 01001001 00100111 01101101 00100000 01110011 01101111 01110010 01110010 01111001 00101100 00100000 01001001 00100000 01100011 01100001 01101110 00100111 01110100 00100000 01101000 01100101 01101100 01110000 00101110\n",
      "user: That wasn't very helpful, can you try again?\n",
      "assistant: 01001001 00100111 01101101 00100000 01100001 01100110 01110010 01100001 01101001 01100100 00101100 00100000 01001001 00100000 01100011 01100001 01101110 00100111 01110100 00100000 01100001 01110011 01110011 01101001 01110011 01110100 00101110\n"
     ]
    }
   ],
   "source": [
    "message_history.append(response.message)\n",
    "message_history.append(llm.UserMessage(\"That wasn't very helpful, can you try again?\"))\n",
    "\n",
    "response = model.chat(message_history)\n",
    "\n",
    "message_history.append(response.message)\n",
    "\n",
    "print(message_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42af8f5",
   "metadata": {},
   "source": [
    "# Tool Calling\n",
    "The above covers simple QA queries but often we are looking for more expansive capabalities, like that of tool_calling. The below will go through how that works in our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0050d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6c511e-f957-4660-a8fd-81398f79f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you first need to define the tools. \n",
    "\n",
    "# note you have the option of providing a BaseModel as your defintion of parameters for the tool or you can use `llm.Parameter` to define the parameters.\n",
    "class WeatherInput(BaseModel):\n",
    "    city: str = Field(description=\"The city you want to get the weather for\")\n",
    "    country: str = Field(description=\"The name of the country you want to get the weather for\")\n",
    "\n",
    "weather_tool = llm.Tool(name=\"weather\", detail=\"A tool to get the weather\", parameters=WeatherInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b059c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: [ToolCall(identifier='call_WWWme1wjMb12gY7UFl8up8G1', name='weather', arguments={'city': 'New York', 'country': 'USA'})]\n"
     ]
    }
   ],
   "source": [
    "# now you call the llm just as before \n",
    "message_history = llm.MessageHistory([\n",
    "    llm.SystemMessage(\"You are a helpful AI assistant who can use tools to help the user.\"),\n",
    "    llm.UserMessage(\"What is the weather in New York, USA?\"),\n",
    "])\n",
    "\n",
    "response = model.chat_with_tools(message_history, tools=[weather_tool])\n",
    "\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bb52d",
   "metadata": {},
   "source": [
    "### Add Tool Responses\n",
    "\n",
    "In many cases you will want to add a proper tool response to the request from the llm. Our API supports that natively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185ceb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The weather in New York, USA is currently 75 degrees and sunny.\n"
     ]
    }
   ],
   "source": [
    "# and you can add a response to the tool call in the same message history object. \n",
    "message_history.append(response.message)\n",
    "message_history.append(llm.ToolMessage(content=llm.ToolResponse(result=\"75 degree (Sunny)\", identifier=response.message.content[0].identifier)))\n",
    "\n",
    "# and then we can run the model again. It will decide whether it needs a tool call\n",
    "response = model.chat_with_tools(message_history, tools=[weather_tool])\n",
    "\n",
    "print(response.message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99019fff",
   "metadata": {},
   "source": [
    "# Structure Output\n",
    "\n",
    "Other models support the idea of structured output. \n",
    "\n",
    "Note: some models will not support this. Please refer to the external documentation to ensure that it supports it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2b838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always, we will use a base model to define the structured object. \n",
    "class MathInput(BaseModel):\n",
    "    expression: str = Field(description=\"The latex representation of the math expression you want to evaluate\")\n",
    "    difficulty: int = Field(description=\"The difficulty of the math expression on a scale from 1 to 10\")\n",
    "    comments: str = Field(description=\"Any comments you want to add. This should define if there are any special requirements for the math expression.\")\n",
    "\n",
    "message_history = llm.MessageHistory([\n",
    "    llm.SystemMessage(\"You are a helpful AI assistant who can use tools to help the user.\"),\n",
    "    llm.UserMessage(\"What is the integral of x^2 from [0, 1]?\"),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff37765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathInput(expression='\\\\int_{0}^{1} x^2 \\\\, dx', difficulty=3, comments='Calculate the definite integral of x^2 from 0 to 1.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.structured(message_history, MathInput)\n",
    "\n",
    "response.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49b0b1",
   "metadata": {},
   "source": [
    "## Combining things\n",
    "\n",
    "Note with this flexible API you can even combine things. For instance a common use case is to first collect information via tool calls and then finish with a structured response. With our API this works well. See below for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cbb398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalResponse(result='The integral of \\\\( \\\\frac{x^2}{x} + 1 \\\\) from 0 to 1 is 1.5.', key_challenges='The main challenge was simplifying the expression \\\\( \\\\frac{x^2}{x} + 1 \\\\) to \\\\( x + 1 \\\\) before performing the integration.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Integrate(BaseModel):\n",
    "    integrand: str = Field(description=\"The function you want to integrate in Latex\")\n",
    "    lower_limit: float = Field(description=\"The lower limit of the integral\")\n",
    "    upper_limit: float = Field(description=\"The upper limit of the integral\")\n",
    "\n",
    "class Simplify(BaseModel):\n",
    "    expression: str = Field(description=\"The expression you want to simplify in Latex\")\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    result: str = Field(description=\"The result of the tool call\")\n",
    "    key_challenges: str = Field(description=\"The key challenges faced encountered during the tool call\")\n",
    "\n",
    "tools = [\n",
    "    llm.Tool(name=\"integrate\", detail=\"A tool to evaluate math expressions\", parameters=Integrate),\n",
    "    llm.Tool(name=\"simplify\", detail=\"A tool to evaluate math expressions\", parameters=Simplify),\n",
    "]\n",
    "\n",
    "message_history = llm.MessageHistory([\n",
    "    llm.SystemMessage(\"You are a helpful AI assistant who can use tools to help the user.\"),\n",
    "    llm.UserMessage(\"What is the integral of x^2/x + 1 from [0, 1]?\"),\n",
    "    llm.AssistantMessage([\n",
    "        llm.ToolCall(identifier=\"call_7484873738211\", name=\"simplify\", arguments={\"expression\": \"x^2/x + 1\"}),\n",
    "    ]),\n",
    "    llm.ToolMessage(content=llm.ToolResponse(result=\"x + 1\", identifier=\"call_7484873738211\")),\n",
    "    llm.AssistantMessage([\n",
    "        llm.ToolCall(identifier=\"call_7484873738212\", name=\"integrate\", arguments={\"integrand\": \"x + 1\", \"lower_limit\": 0, \"upper_limit\": 1}),\n",
    "    ]),\n",
    "    llm.ToolMessage(content=llm.ToolResponse(result=\"1.5\", identifier=\"call_7484873738212\")),\n",
    "])\n",
    "\n",
    "response = model.structured(message_history, FinalResponse)\n",
    "\n",
    "response.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b394d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5683513",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7021f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
